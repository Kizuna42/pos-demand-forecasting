"""
ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ  ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å…¨ä½“ã®åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã€
ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‹ã‚‰æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¾ã§çµ±åˆçš„ã«å‡¦ç†ã—ã¾ã™ã€‚

Requirements: 8.1, 8.2, 8.3, 8.4
- 8.1: é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°å‡ºåŠ›æ©Ÿèƒ½
- 8.2: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†æ©Ÿèƒ½
- 8.3: é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµæœä¿å­˜æ©Ÿèƒ½
- 8.4: åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°å‡ºåŠ›æ©Ÿèƒ½
"""

import argparse
import os
import sys
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.demand_forecasting.core.data_processor import DataProcessor
from src.demand_forecasting.core.demand_analyzer import DemandCurveAnalyzer
from src.demand_forecasting.core.feature_engineer import FeatureEngineer
from src.demand_forecasting.core.model_builder import ModelBuilder
from src.demand_forecasting.reports.report_generator import ReportGenerator
from src.demand_forecasting.utils.config import Config
from src.demand_forecasting.utils.exceptions import (
    ConfigurationError,
    DataProcessingError,
    DemandForecastingError,
    FeatureEngineeringError,
    ModelBuildingError,
    ReportGenerationError,
    VisualizationError,
)
from src.demand_forecasting.utils.logger import Logger
from src.demand_forecasting.utils.quality_evaluator import QualityEvaluator
from src.demand_forecasting.visualization.want_plotter import WantPlotter


class DemandForecastingPipeline:
    """
    éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

    å…¨ä½“ã®åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ±åˆã—ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã€
    é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµæœä¿å­˜ã€åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    def __init__(self, config_path: Optional[str] = None):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Raises:
            ConfigurationError: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        try:
            # è¨­å®šç®¡ç†ã®åˆæœŸåŒ–
            self.config = Config(config_path)
            self.logger = Logger(self.config.get_logging_config()).get_logger("main")

            # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
            self.start_time = datetime.now()
            self.logger.info(f"éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–é–‹å§‹: {self.start_time}")

            # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            self._ensure_directories()

            # è¨­å®šå€¤ã®æ¤œè¨¼
            self._validate_configuration()

            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self._initialize_components()

            self.logger.info("éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            error_msg = f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}"
            if hasattr(self, "logger"):
                self.logger.error(error_msg)
                self.logger.error(f"è©³ç´°: {traceback.format_exc()}")
            else:
                print(f"âŒ {error_msg}")
                print(f"è©³ç´°: {traceback.format_exc()}")
            raise ConfigurationError(error_msg) from e

    def _ensure_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        try:
            # åŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            required_dirs = [
                "data/processed",
                "models",
                "reports",
                "output/visualizations",
                "logs",
            ]

            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¿½åŠ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
            viz_config = self.config.get_visualization_config()
            if viz_config.get("output_dir"):
                required_dirs.append(viz_config["output_dir"])

            processed_path = self.config.get("data.processed_data_path")
            if processed_path:
                required_dirs.append(processed_path)

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            for dir_path in required_dirs:
                Path(dir_path).mkdir(parents=True, exist_ok=True)

            self.logger.info(f"å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª/ä½œæˆã—ã¾ã—ãŸ: {required_dirs}")

        except Exception as e:
            raise ConfigurationError(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}") from e

    def _validate_configuration(self):
        """è¨­å®šå€¤ã®æ¤œè¨¼"""
        try:
            # å¿…é ˆè¨­å®šé …ç›®ã®ç¢ºèª
            required_configs = [
                ("data.raw_data_path", "ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"),
                ("data.encoding", "æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"),
                ("model.algorithm", "ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "),
                ("quality.thresholds", "å“è³ªé–¾å€¤"),
            ]

            missing_configs = []
            for config_key, description in required_configs:
                if self.config.get(config_key) is None:
                    missing_configs.append(f"{config_key} ({description})")

            if missing_configs:
                raise ConfigurationError(f"å¿…é ˆè¨­å®šé …ç›®ãŒä¸è¶³: {', '.join(missing_configs)}")

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            raw_data_path = self.config.get("data.raw_data_path")
            if not Path(raw_data_path).exists():
                raise ConfigurationError(f"ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {raw_data_path}")

            self.logger.info("è¨­å®šå€¤ã®æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸ")

        except Exception as e:
            if isinstance(e, ConfigurationError):
                raise
            raise ConfigurationError(f"è¨­å®šå€¤æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}") from e

    def _initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        try:
            self.logger.info("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–é–‹å§‹")

            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é †æ¬¡åˆæœŸåŒ–
            components = [
                ("data_processor", DataProcessor),
                ("feature_engineer", FeatureEngineer),
                ("model_builder", ModelBuilder),
                ("demand_analyzer", DemandCurveAnalyzer),
                ("quality_evaluator", QualityEvaluator),
                ("plotter", WantPlotter),
                ("report_generator", ReportGenerator),
            ]

            for component_name, component_class in components:
                try:
                    setattr(self, component_name, component_class(self.config))
                    self.logger.debug(f"{component_name} åˆæœŸåŒ–å®Œäº†")
                except Exception as e:
                    raise ConfigurationError(f"{component_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}") from e

            self.logger.info("å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            if isinstance(e, ConfigurationError):
                raise
            raise ConfigurationError(f"ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}") from e

    def run_full_analysis(
        self, target_products: Optional[List[str]] = None, max_products: int = 10
    ) -> Dict[str, Any]:
        """
        å…¨ä½“åˆ†æã‚’å®Ÿè¡Œ

        è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã€é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµæœä¿å­˜ã€
        åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æä¾›ã—ã¾ã™ã€‚

        Args:
            target_products: å¯¾è±¡å•†å“ãƒªã‚¹ãƒˆ
            max_products: æœ€å¤§å‡¦ç†å•†å“æ•°

        Returns:
            åˆ†æçµæœè¾æ›¸

        Raises:
            DemandForecastingError: åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        analysis_start_time = datetime.now()
        self.logger.info(f"éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹: {analysis_start_time}")

        # åˆ†æçµæœã‚’æ ¼ç´ã™ã‚‹å¤‰æ•°ã‚’åˆæœŸåŒ–
        analysis_results = []
        quality_report = {}
        visualization_files = []
        report_files = []

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†")
            self.logger.info("=" * 50)

            try:
                raw_data = self.data_processor.load_raw_data()
                self.logger.info(f"ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(raw_data)} ãƒ¬ã‚³ãƒ¼ãƒ‰")

                clean_data = self.data_processor.clean_data(raw_data)
                self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {len(clean_data)} ãƒ¬ã‚³ãƒ¼ãƒ‰")

                # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                processed_data_path = Path(
                    self.config.get("data.processed_data_path", "data/processed")
                )
                processed_file = (
                    processed_data_path
                    / f"cleaned_data_{analysis_start_time.strftime('%Y%m%d_%H%M%S')}.csv"
                )
                clean_data.to_csv(processed_file, index=False, encoding="utf-8")
                self.logger.info(f"å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {processed_file}")

            except Exception as e:
                raise DataProcessingError(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}") from e

            # ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
            self.logger.info("=" * 50)

            try:
                baseline_features = self.feature_engineer.create_baseline_features(clean_data)
                self.logger.info(
                    f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {baseline_features.shape[1]} ç‰¹å¾´é‡"
                )

                time_features = self.feature_engineer.add_time_features(baseline_features)
                self.logger.info(f"æ™‚é–“ç‰¹å¾´é‡è¿½åŠ å®Œäº†: {time_features.shape[1]} ç‰¹å¾´é‡")

                weather_features = self.feature_engineer.integrate_weather_features(time_features)
                self.logger.info(f"æ°—è±¡ç‰¹å¾´é‡çµ±åˆå®Œäº†: {weather_features.shape[1]} ç‰¹å¾´é‡")

                # é«˜åº¦ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡è¿½åŠ 
                final_features = self.feature_engineer.add_advanced_time_series_features(
                    weather_features
                )
                self.logger.info(f"æœ€çµ‚ç‰¹å¾´é‡ã‚»ãƒƒãƒˆå®Œæˆ: {final_features.shape[1]} ç‰¹å¾´é‡")

                # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                features_file = (
                    processed_data_path
                    / f"features_{analysis_start_time.strftime('%Y%m%d_%H%M%S')}.csv"
                )
                final_features.to_csv(features_file, index=False, encoding="utf-8")
                self.logger.info(f"ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {features_file}")

            except Exception as e:
                raise FeatureEngineeringError(
                    f"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}"
                ) from e

            # ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†æå¯¾è±¡å•†å“ã®æ±ºå®š
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†æå¯¾è±¡å•†å“ã®æ±ºå®š")
            self.logger.info("=" * 50)

            try:
                if target_products is None:
                    target_products = self.data_processor.stratified_product_sampling(
                        final_features, max_products=max_products
                    )

                self.logger.info(f"åˆ†æå¯¾è±¡å•†å“æ±ºå®š: {len(target_products)} å•†å“")
                for i, product in enumerate(target_products, 1):
                    self.logger.info(f"  {i:2d}. {product}")

            except Exception as e:
                raise DataProcessingError(f"å•†å“é¸æŠã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}") from e

            # ã‚¹ãƒ†ãƒƒãƒ—4: å•†å“åˆ¥åˆ†æå®Ÿè¡Œ
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—4: å•†å“åˆ¥åˆ†æå®Ÿè¡Œ")
            self.logger.info("=" * 50)

            successful_analyses = 0
            failed_analyses = 0

            for i, product in enumerate(target_products, 1):
                self.logger.info(f"å•†å“åˆ†æ {i}/{len(target_products)}: {product}")

                try:
                    # å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    product_data = final_features[final_features["å•†å“åç§°"] == product].copy()

                    if len(product_data) < 100:
                        self.logger.warning(
                            f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Š{product}ã‚’ã‚¹ã‚­ãƒƒãƒ— (ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(product_data)})"
                        )
                        failed_analyses += 1
                        continue

                    # æ™‚ç³»åˆ—å“è³ªãƒã‚§ãƒƒã‚¯
                    if not self._validate_time_series_quality(product_data, product):
                        self.logger.warning(f"æ™‚ç³»åˆ—å“è³ªä¸è‰¯ã«ã‚ˆã‚Š{product}ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        failed_analyses += 1
                        continue

                    result = self._analyze_single_product(product, product_data, final_features)
                    if result:
                        analysis_results.append(result)
                        successful_analyses += 1
                        self.logger.info(
                            f"  âœ… {product}: RÂ²={result.get('test_metrics', {}).get('r2_score', 0):.3f}"
                        )
                    else:
                        failed_analyses += 1
                        self.logger.warning(f"  âŒ {product}: åˆ†æçµæœãªã—")

                except Exception as e:
                    failed_analyses += 1
                    self.logger.error(f"  âŒ {product}: åˆ†æã‚¨ãƒ©ãƒ¼ - {e}")
                    self.logger.debug(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
                    continue

            self.logger.info(f"å•†å“åˆ¥åˆ†æå®Œäº†: æˆåŠŸ={successful_analyses}, å¤±æ•—={failed_analyses}")

            # ã‚¹ãƒ†ãƒƒãƒ—5: å“è³ªè©•ä¾¡
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—5: å“è³ªè©•ä¾¡")
            self.logger.info("=" * 50)

            try:
                quality_report = self.quality_evaluator.create_quality_report(analysis_results)
                self.logger.info("å“è³ªè©•ä¾¡å®Œäº†")

            except Exception as e:
                self.logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                # å“è³ªè©•ä¾¡ãŒå¤±æ•—ã—ã¦ã‚‚åˆ†æã¯ç¶™ç¶š
                quality_report = {"summary": {"success_rate": 0.0, "average_r2": 0.0}}

            # ã‚¹ãƒ†ãƒƒãƒ—6: å¯è¦–åŒ–ç”Ÿæˆ
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—6: å¯è¦–åŒ–ç”Ÿæˆ")
            self.logger.info("=" * 50)

            try:
                visualization_files = self._generate_visualizations(
                    analysis_results, quality_report
                )
                self.logger.info(f"å¯è¦–åŒ–ç”Ÿæˆå®Œäº†: {len(visualization_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

            except Exception as e:
                self.logger.error(f"å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                self.logger.debug(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
                # å¯è¦–åŒ–ãŒå¤±æ•—ã—ã¦ã‚‚åˆ†æã¯ç¶™ç¶š
                visualization_files = []

            # ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self.logger.info("=" * 50)
            self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
            self.logger.info("=" * 50)

            try:
                report_files = self._generate_reports(analysis_results, quality_report)
                self.logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {len(report_files)} ãƒ•ã‚¡ã‚¤ãƒ«")

            except Exception as e:
                self.logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                self.logger.debug(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
                # ãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆãŒå¤±æ•—ã—ã¦ã‚‚åˆ†æã¯ç¶™ç¶š
                report_files = []

            # çµæœçµ±åˆ
            analysis_end_time = datetime.now()
            execution_time = (analysis_end_time - analysis_start_time).total_seconds()

            final_results = {
                "analysis_results": analysis_results,
                "quality_report": quality_report,
                "visualization_files": visualization_files,
                "report_files": report_files,
                "execution_info": {
                    "start_time": analysis_start_time.isoformat(),
                    "end_time": analysis_end_time.isoformat(),
                    "execution_time_seconds": execution_time,
                    "successful_analyses": successful_analyses,
                    "failed_analyses": failed_analyses,
                },
                "summary": {
                    "total_products_analyzed": len(analysis_results),
                    "success_rate": quality_report.get("summary", {}).get("success_rate", 0.0),
                    "average_r2": quality_report.get("summary", {}).get("average_r2", 0.0),
                },
            }

            self.logger.info("=" * 50)
            self.logger.info("éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
            self.logger.info("=" * 50)
            self._log_summary(final_results)

            return final_results

        except Exception as e:
            # åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            error_msg = f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")

            # éƒ¨åˆ†çš„ãªçµæœã§ã‚‚è¿”å´
            partial_results = {
                "analysis_results": analysis_results,
                "quality_report": quality_report,
                "visualization_files": visualization_files,
                "report_files": report_files,
                "error": error_msg,
                "summary": {
                    "total_products_analyzed": len(analysis_results),
                    "success_rate": 0.0,
                    "average_r2": 0.0,
                },
            }

            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å¯èƒ½ãªé™ã‚Šçµæœã‚’ä¿å­˜
            try:
                if analysis_results:
                    self.logger.info("ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®éƒ¨åˆ†çš„çµæœã‚’ä¿å­˜ä¸­...")
                    self._generate_reports(analysis_results, quality_report or {})
            except Exception as save_error:
                self.logger.error(f"éƒ¨åˆ†çš„çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")

            raise DemandForecastingError(error_msg) from e

    def _analyze_single_product(
        self, product: str, product_data: pd.DataFrame, full_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """å˜ä¸€å•†å“ã®åˆ†æ"""
        try:
            # ç‰¹å¾´é‡é¸æŠ
            target_column = "æ•°é‡"
            if target_column not in product_data.columns:
                self.logger.warning(f"{product}: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—'{target_column}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None

            # å¼·åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡é¸æŠã‚’å®Ÿè¡Œ
            selected_features = self.feature_engineer.select_features(
                product_data, target_column, max_features=25
            )

            if len(selected_features) < 3:
                self.logger.warning(
                    f"{product}: ç‰¹å¾´é‡é¸æŠå¾Œã®æ•°ãŒä¸è¶³ ({len(selected_features)}å€‹)"
                )
                return None

            X = product_data[selected_features]
            y = product_data[target_column]

            self.logger.info(f"{product}: é¸æŠç‰¹å¾´é‡æ•°={len(selected_features)}")

            # Phase 3: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            model_results = self.model_builder.train_with_cv(X, y, model_type="ensemble")

            # å“è³ªãƒ¬ãƒ™ãƒ«è©•ä¾¡
            quality_level = self.quality_evaluator.evaluate_quality_level(
                model_results["test_metrics"]["r2_score"]
            )

            # å®Ÿç”¨åŒ–æº–å‚™çŠ¶æ³è©•ä¾¡
            implementation_readiness = self.quality_evaluator.assess_implementation_readiness(
                model_results["test_metrics"]
            )

            # éœ€è¦æ›²ç·šåˆ†æ
            demand_results = None
            try:
                demand_results = self.demand_analyzer.analyze_demand_curve(product_data, product)
            except Exception as e:
                self.logger.warning(f"{product}: éœ€è¦æ›²ç·šåˆ†æå¤±æ•—: {e}")

            # å•†å“ã‚«ãƒ†ã‚´ãƒªå–å¾—
            category = (
                product_data["å•†å“ã‚«ãƒ†ã‚´ãƒª"].iloc[0]
                if "å•†å“ã‚«ãƒ†ã‚´ãƒª" in product_data.columns
                else "ãã®ä»–"
            )

            # çµæœçµ±åˆ
            result = {
                "product_name": product,
                "category": category,
                "quality_level": quality_level,
                "implementation_readiness": implementation_readiness,
                "test_metrics": model_results["test_metrics"],
                "cv_scores": model_results["cv_scores"],
                "overfitting_score": model_results["overfitting_score"],
                "feature_importance": model_results["feature_importance"],
                "model": model_results["model"],
                "feature_names": model_results["feature_names"],
            }

            if demand_results:
                result["demand_results"] = demand_results

            return result

        except Exception as e:
            self.logger.error(f"å•†å“{product}ã®åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _generate_visualizations(
        self, analysis_results: List[Dict[str, Any]], quality_report: Dict[str, Any]
    ) -> List[str]:
        """
        å¯è¦–åŒ–ã‚’ç”Ÿæˆ

        é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµæœä¿å­˜ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æä¾›ã—ã¾ã™ã€‚
        """
        visualization_files = []

        if not analysis_results:
            self.logger.warning("åˆ†æçµæœãŒãªã„ãŸã‚å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return visualization_files

        try:
            # å¯è¦–åŒ–å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿
            viz_config = self.config.get_visualization_config()
            output_dir = Path(viz_config.get("output_dir", "output/visualizations"))
            output_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
            try:
                quality_dashboard_path = self.plotter.create_quality_dashboard(
                    quality_report.get("summary", {})
                )
                if quality_dashboard_path:
                    visualization_files.append(quality_dashboard_path)
                    self.logger.info(f"å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆ: {quality_dashboard_path}")
            except Exception as e:
                self.logger.error(f"å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

            # å€‹åˆ¥å•†å“ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¸Šä½5å•†å“ã®ã¿ï¼‰
            sorted_results = sorted(
                analysis_results,
                key=lambda x: x.get("test_metrics", {}).get("r2_score", 0),
                reverse=True,
            )[:5]

            self.logger.info(f"ä¸Šä½{len(sorted_results)}å•†å“ã®å€‹åˆ¥ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")

            for i, result in enumerate(sorted_results, 1):
                product_name = result.get("product_name", f"product_{i}")

                try:
                    # éœ€è¦æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
                    if "demand_results" in result:
                        demand_plot_path = self.plotter.create_demand_curve_plot(
                            result["demand_results"]
                        )
                        if demand_plot_path:
                            visualization_files.append(demand_plot_path)
                            self.logger.info(f"éœ€è¦æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ: {product_name}")
                except Exception as e:
                    self.logger.error(f"éœ€è¦æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({product_name}): {e}")

                try:
                    # ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆ
                    if "feature_importance" in result:
                        importance_plot_path = self.plotter.create_feature_importance_plot(
                            result["feature_importance"], product_name
                        )
                        if importance_plot_path:
                            visualization_files.append(importance_plot_path)
                            self.logger.info(f"ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ: {product_name}")
                except Exception as e:
                    self.logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({product_name}): {e}")

            # å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            if visualization_files:
                viz_list_file = output_dir / f"visualization_files_{timestamp}.txt"
                with open(viz_list_file, "w", encoding="utf-8") as f:
                    f.write(f"å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ - {timestamp}\n")
                    f.write("=" * 50 + "\n")
                    for i, file_path in enumerate(visualization_files, 1):
                        f.write(f"{i:2d}. {file_path}\n")

                self.logger.info(f"å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ä¿å­˜: {viz_list_file}")

        except Exception as e:
            error_msg = f"å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.logger.debug(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            raise VisualizationError(error_msg) from e

        return visualization_files

    def _generate_reports(
        self, analysis_results: List[Dict[str, Any]], quality_report: Dict[str, Any]
    ) -> List[str]:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµæœä¿å­˜ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æä¾›ã—ã¾ã™ã€‚
        """
        report_files = []

        if not analysis_results:
            self.logger.warning("åˆ†æçµæœãŒãªã„ãŸã‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return report_files

        try:
            # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿
            reports_dir = Path("reports")
            reports_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            try:
                self.logger.info("Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
                markdown_report = self.report_generator.generate_markdown_report(
                    analysis_results, quality_report
                )
                markdown_path = self.report_generator.save_markdown_report(markdown_report)
                if markdown_path:
                    report_files.append(markdown_path)
                    self.logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {markdown_path}")
            except Exception as e:
                self.logger.error(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

            # CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            try:
                self.logger.info("CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
                csv_paths = self.report_generator.generate_csv_reports(analysis_results)
                if csv_paths:
                    report_files.extend(csv_paths)
                    self.logger.info(f"CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {len(csv_paths)} ãƒ•ã‚¡ã‚¤ãƒ«")
                    for csv_path in csv_paths:
                        self.logger.info(f"  - {csv_path}")
            except Exception as e:
                self.logger.error(f"CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            try:
                models_dir = Path("models")
                models_dir.mkdir(parents=True, exist_ok=True)

                model_files = []
                for i, result in enumerate(analysis_results):
                    if "model" in result and result["model"] is not None:
                        product_name = result.get("product_name", f"product_{i}")
                        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›
                        safe_name = "".join(
                            c if c.isalnum() or c in "._-" else "_" for c in product_name
                        )
                        model_file = models_dir / f"model_{safe_name}_{timestamp}.pkl"

                        try:
                            import pickle

                            with open(model_file, "wb") as f:
                                pickle.dump(result["model"], f)
                            model_files.append(str(model_file))
                            self.logger.debug(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")
                        except Exception as e:
                            self.logger.warning(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({product_name}): {e}")

                if model_files:
                    self.logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {len(model_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
                    report_files.extend(model_files)

            except Exception as e:
                self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            if report_files:
                report_list_file = reports_dir / f"report_files_{timestamp}.txt"
                with open(report_list_file, "w", encoding="utf-8") as f:
                    f.write(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ - {timestamp}\n")
                    f.write("=" * 50 + "\n")
                    for i, file_path in enumerate(report_files, 1):
                        f.write(f"{i:2d}. {file_path}\n")

                self.logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ä¿å­˜: {report_list_file}")

        except Exception as e:
            error_msg = f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.logger.debug(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            raise ReportGenerationError(error_msg) from e

        return report_files

    def _validate_time_series_quality(self, product_data: pd.DataFrame, product_name: str) -> bool:
        """
        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼

        Args:
            product_data: å•†å“ãƒ‡ãƒ¼ã‚¿
            product_name: å•†å“å

        Returns:
            å“è³ªãŒååˆ†ãªã‚‰Trueã€ä¸ååˆ†ãªã‚‰False
        """
        try:
            # æ—¥ä»˜åˆ—ã®å­˜åœ¨ç¢ºèª
            if "å¹´æœˆæ—¥" not in product_data.columns:
                return False

            # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
            product_data_sorted = product_data.sort_values("å¹´æœˆæ—¥")

            # 1. æ™‚ç³»åˆ—é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ï¼šãƒ‡ãƒ¼ã‚¿ã®æœŸé–“ã‚’ãƒã‚§ãƒƒã‚¯
            date_range = (
                product_data_sorted["å¹´æœˆæ—¥"].max() - product_data_sorted["å¹´æœˆæ—¥"].min()
            ).days
            if date_range < 90:  # 3ãƒ¶æœˆæœªæº€ã®ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–
                self.logger.warning(f"{product_name}: ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒçŸ­ã™ãã¾ã™ ({date_range}æ—¥)")
                return False

            # 2. ãƒ‡ãƒ¼ã‚¿å¯†åº¦ãƒã‚§ãƒƒã‚¯ï¼šæœŸé–“ã«å¯¾ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®å¯†åº¦
            expected_records = date_range * 0.3  # 3æ—¥ã«1å›ç¨‹åº¦ã®å£²ä¸Šã‚’æœŸå¾…
            if len(product_data) < expected_records:
                self.logger.warning(
                    f"{product_name}: ãƒ‡ãƒ¼ã‚¿å¯†åº¦ãŒä½ã™ãã¾ã™ (æœŸé–“:{date_range}æ—¥, ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°:{len(product_data)})"
                )
                return False

            # 3. å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ï¼šæœˆæ¬¡å£²ä¸Šã®åˆ†æ•£ã‚’ãƒã‚§ãƒƒã‚¯
            if "month" in product_data.columns or "æœˆ" in product_data.columns:
                month_col = "month" if "month" in product_data.columns else "æœˆ"
                monthly_sales = product_data.groupby(month_col)["æ•°é‡"].sum()

                # æœˆæ¬¡å£²ä¸Šã®å¤‰å‹•ä¿‚æ•°ï¼ˆCVï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                cv = (
                    monthly_sales.std() / monthly_sales.mean()
                    if monthly_sales.mean() > 0
                    else float("inf")
                )
                if cv > 2.0:  # å¤‰å‹•ä¿‚æ•°ãŒ2.0ã‚’è¶…ãˆã‚‹å ´åˆã¯ä¸å®‰å®šã™ãã‚‹
                    self.logger.warning(
                        f"{product_name}: æœˆæ¬¡å£²ä¸Šã®ä¸å®‰å®šæ€§ãŒé«˜ã™ãã¾ã™ (CV: {cv:.2f})"
                    )
                    return False

            # 4. ã‚¼ãƒ­å£²ä¸ŠæœŸé–“ãƒã‚§ãƒƒã‚¯ï¼šé€£ç¶šã™ã‚‹ã‚¼ãƒ­å£²ä¸Šã®æœŸé–“ã‚’ãƒã‚§ãƒƒã‚¯
            zero_sales_ratio = (product_data["æ•°é‡"] == 0).sum() / len(product_data)
            if zero_sales_ratio > 0.5:  # ã‚¼ãƒ­å£²ä¸ŠãŒ50%ã‚’è¶…ãˆã‚‹å ´åˆ
                self.logger.warning(
                    f"{product_name}: ã‚¼ãƒ­å£²ä¸Šã®æ¯”ç‡ãŒé«˜ã™ãã¾ã™ ({zero_sales_ratio:.1%})"
                )
                return False

            return True

        except Exception as e:
            self.logger.error(f"{product_name}: æ™‚ç³»åˆ—å“è³ªæ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _log_summary(self, results: Dict[str, Any]):
        """
        çµæœã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›

        åŒ…æ‹¬çš„ãªå®Ÿè¡Œçµæœã®è¦ç´„ã‚’æä¾›ã—ã¾ã™ã€‚
        """
        summary = results["summary"]
        execution_info = results.get("execution_info", {})

        self.logger.info("=" * 60)
        self.logger.info("ğŸ¯ éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        self.logger.info("=" * 60)

        # å®Ÿè¡Œæƒ…å ±
        if execution_info:
            self.logger.info("ğŸ“… å®Ÿè¡Œæƒ…å ±:")
            if "start_time" in execution_info:
                self.logger.info(f"   é–‹å§‹æ™‚åˆ»: {execution_info['start_time']}")
            if "end_time" in execution_info:
                self.logger.info(f"   çµ‚äº†æ™‚åˆ»: {execution_info['end_time']}")
            if "execution_time_seconds" in execution_info:
                exec_time = execution_info["execution_time_seconds"]
                hours = int(exec_time // 3600)
                minutes = int((exec_time % 3600) // 60)
                seconds = int(exec_time % 60)
                self.logger.info(f"   å®Ÿè¡Œæ™‚é–“: {hours:02d}:{minutes:02d}:{seconds:02d}")

            if "successful_analyses" in execution_info and "failed_analyses" in execution_info:
                total = execution_info["successful_analyses"] + execution_info["failed_analyses"]
                success_rate = (
                    execution_info["successful_analyses"] / total * 100 if total > 0 else 0
                )
                self.logger.info(
                    f"   æˆåŠŸ/å¤±æ•—: {execution_info['successful_analyses']}/{execution_info['failed_analyses']} ({success_rate:.1f}%)"
                )

        # åˆ†æçµæœ
        self.logger.info("ğŸ“Š åˆ†æçµæœ:")
        self.logger.info(f"   åˆ†æå•†å“æ•°: {summary['total_products_analyzed']}")
        self.logger.info(f"   æˆåŠŸç‡: {summary['success_rate']*100:.1f}%")
        self.logger.info(f"   å¹³å‡RÂ²ã‚¹ã‚³ã‚¢: {summary['average_r2']:.3f}")

        # å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ
        quality_report = results.get("quality_report", {})
        if "quality_distribution" in quality_report:
            self.logger.info("   å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ:")
            for level, count in quality_report["quality_distribution"].items():
                self.logger.info(f"     {level}: {count} å•†å“")

        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        self.logger.info("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        self.logger.info(f"   å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results['visualization_files'])}")
        self.logger.info(f"   ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results['report_files'])}")

        # è¨­å®šæƒ…å ±
        self.logger.info("âš™ï¸  ä½¿ç”¨è¨­å®š:")
        model_config = self.config.get_model_config()
        self.logger.info(f"   ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {model_config.get('algorithm', 'N/A')}")
        self.logger.info(f"   äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°: {model_config.get('cv_folds', 'N/A')}")

        quality_config = self.config.get_quality_config()
        thresholds = quality_config.get("thresholds", {})
        self.logger.info(
            f"   å“è³ªé–¾å€¤: Premiumâ‰¥{thresholds.get('premium', 'N/A')}, Standardâ‰¥{thresholds.get('standard', 'N/A')}, Basicâ‰¥{thresholds.get('basic', 'N/A')}"
        )

        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        if "error" in results:
            self.logger.error("âŒ ã‚¨ãƒ©ãƒ¼æƒ…å ±:")
            self.logger.error(f"   {results['error']}")

        self.logger.info("=" * 60)


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°

    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã€éœ€è¦äºˆæ¸¬åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°å‡ºåŠ›ã‚’æä¾›ã—ã¾ã™ã€‚
    """
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¨­å®š
    parser = argparse.ArgumentParser(
        description="ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python src/main.py                                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
  python src/main.py --config config/custom.yaml       # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§å®Ÿè¡Œ
  python src/main.py --products "å•†å“A" "å•†å“B"         # ç‰¹å®šå•†å“ã®ã¿åˆ†æ
  python src/main.py --max-products 20 --verbose       # è©³ç´°ãƒ­ã‚°ã§20å•†å“ã¾ã§åˆ†æ
        """,
    )

    parser.add_argument(
        "--config", type=str, help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: config/config.yaml)"
    )
    parser.add_argument(
        "--products", type=str, nargs="+", help="å¯¾è±¡å•†å“ãƒªã‚¹ãƒˆ (æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•é¸æŠ)"
    )
    parser.add_argument(
        "--max-products", type=int, default=10, help="æœ€å¤§å‡¦ç†å•†å“æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)"
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument(
        "--dry-run", action="store_true", help="è¨­å®šç¢ºèªã®ã¿å®Ÿè¡Œï¼ˆå®Ÿéš›ã®åˆ†æã¯è¡Œã‚ãªã„ï¼‰"
    )

    args = parser.parse_args()

    # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»
    start_time = datetime.now()

    # åˆæœŸåŒ–æ®µéšã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    pipeline = None
    try:
        print("ğŸš€ ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        print(f"å®Ÿè¡Œé–‹å§‹æ™‚åˆ»: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if args.config:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {args.config}")
        else:
            print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: config/config.yaml (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)")

        if args.products:
            print(f"å¯¾è±¡å•†å“: {', '.join(args.products)}")
        else:
            print(f"å¯¾è±¡å•†å“: è‡ªå‹•é¸æŠ (æœ€å¤§{args.max_products}å•†å“)")

        print("=" * 50)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        pipeline = DemandForecastingPipeline(args.config)

        # è©³ç´°ãƒ­ã‚°è¨­å®š
        if args.verbose:
            pipeline.logger.setLevel("DEBUG")
            print("ğŸ“ è©³ç´°ãƒ­ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ")

        # Dry run ãƒ¢ãƒ¼ãƒ‰
        if args.dry_run:
            print("ğŸ” è¨­å®šç¢ºèªãƒ¢ãƒ¼ãƒ‰ (å®Ÿéš›ã®åˆ†æã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“)")
            print("\nâœ… è¨­å®šç¢ºèªå®Œäº†!")
            print("è¨­å®šã«å•é¡ŒãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")
            return

        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print()

        # åˆ†æå®Ÿè¡Œ
        print("ğŸ”„ åˆ†æå®Ÿè¡Œä¸­...")
        results = pipeline.run_full_analysis(
            target_products=args.products, max_products=args.max_products
        )

        # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
        end_time = datetime.now()
        execution_time = end_time - start_time

        # çµæœè¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ‰ åˆ†æå®Œäº†!")
        print("=" * 60)

        summary = results["summary"]
        execution_info = results.get("execution_info", {})

        print(f"ğŸ“Š åˆ†æå•†å“æ•°: {summary['total_products_analyzed']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']*100:.1f}%")
        print(f"ğŸ¯ å¹³å‡RÂ²ã‚¹ã‚³ã‚¢: {summary['average_r2']:.3f}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time}")
        print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {len(results['report_files'])}ä»¶")
        print(f"ğŸ“Š å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: {len(results['visualization_files'])}ä»¶")

        # å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒè¡¨ç¤º
        quality_report = results.get("quality_report", {})
        if "quality_distribution" in quality_report:
            print("\nğŸ“‹ å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ:")
            for level, count in quality_report["quality_distribution"].items():
                print(f"   {level}: {count} å•†å“")

        # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        all_files = results["report_files"] + results["visualization_files"]
        if all_files:
            print(f"\nğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« ({len(all_files)}ä»¶):")
            for i, file_path in enumerate(all_files, 1):
                print(f"  {i:2d}. {file_path}")

        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\nğŸŠ ã™ã¹ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰
        if summary["success_rate"] > 0.5:
            sys.exit(0)  # æˆåŠŸ
        else:
            print("âš ï¸  æˆåŠŸç‡ãŒä½ã„ãŸã‚ã€è¨­å®šã‚„ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(2)  # è­¦å‘Š

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)

    except ConfigurationError as e:
        print(f"\nâŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ’¡ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    except (DataProcessingError, FeatureEngineeringError, ModelBuildingError) as e:
        print(f"\nâŒ åˆ†æå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚„å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        if pipeline and hasattr(pipeline, "logger"):
            pipeline.logger.error(f"åˆ†æå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    except (VisualizationError, ReportGenerationError) as e:
        print(f"\nâš ï¸  å‡ºåŠ›ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ’¡ åˆ†æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ä¸€éƒ¨ã®å‡ºåŠ›ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        if pipeline and hasattr(pipeline, "logger"):
            pipeline.logger.warning(f"å‡ºåŠ›ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(2)

    except DemandForecastingError as e:
        print(f"\nâŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        if pipeline and hasattr(pipeline, "logger"):
            pipeline.logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    except Exception as e:
        print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")
        if pipeline and hasattr(pipeline, "logger"):
            pipeline.logger.critical(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            pipeline.logger.critical(f"è©³ç´°: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
