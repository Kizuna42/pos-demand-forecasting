{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ  ä½¿ç”¨ä¾‹\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ ç›®æ¬¡\n",
    "1. [ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)\n",
    "2. [åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•](#åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•)\n",
    "3. [å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½¿ç”¨](#å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½¿ç”¨)\n",
    "4. [ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹](#ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹)\n",
    "5. [çµæœã®åˆ†æã¨è§£é‡ˆ](#çµæœã®åˆ†æã¨è§£é‡ˆ)\n",
    "6. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from src.main import DemandForecastingPipeline\n",
    "from src.demand_forecasting.core.data_processor import DataProcessor\n",
    "from src.demand_forecasting.core.feature_engineer import FeatureEngineer\n",
    "from src.demand_forecasting.core.model_builder import ModelBuilder\n",
    "from src.demand_forecasting.utils.config import Config\n",
    "from src.demand_forecasting.utils.logger import Logger\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "### 2.1 å…¨ä½“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "config_path = \"../config/config.yaml\"\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "data_path = \"../data/raw/ç´å“ç”¨_20240101_20241231_ç”Ÿé®®å…¨å“data.csv\"\n",
    "if not Path(data_path).exists():\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™...\")\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯ä¸è¦ï¼‰\n",
    "    sample_data = pd.DataFrame({\n",
    "        'å•†å“ã‚³ãƒ¼ãƒ‰': ['001', '002', '003'] * 100,\n",
    "        'å•†å“åç§°': ['ã‚Šã‚“ã”', 'ã‚­ãƒ£ãƒ™ãƒ„', 'ç‰›è‚‰'] * 100,\n",
    "        'å¹´æœˆæ—¥': pd.date_range('2024-01-01', periods=300).strftime('%Y-%m-%d'),\n",
    "        'é‡‘é¡': np.random.randint(100, 1000, 300),\n",
    "        'æ•°é‡': np.random.randint(1, 10, 300),\n",
    "        'å¹³å‡ä¾¡æ ¼': np.random.randint(50, 200, 300)\n",
    "    })\n",
    "    os.makedirs(Path(data_path).parent, exist_ok=True)\n",
    "    sample_data.to_csv(data_path, index=False, encoding='shift_jis')\n",
    "    print(\"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–\n",
    "pipeline = DemandForecastingPipeline(config_path)\n",
    "\n",
    "# å…¨ä½“åˆ†æã®å®Ÿè¡Œï¼ˆå•†å“æ•°ã‚’åˆ¶é™ã—ã¦ãƒ‡ãƒ¢å®Ÿè¡Œï¼‰\n",
    "print(\"ğŸš€ éœ€è¦äºˆæ¸¬åˆ†æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "results = pipeline.run_full_analysis(max_products=5)\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º\n",
    "print(\"\\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼\")\n",
    "print(f\"åˆ†æå•†å“æ•°: {results['summary']['total_products_analyzed']}\")\n",
    "print(f\"æˆåŠŸç‡: {results['summary']['success_rate']*100:.1f}%\")\n",
    "print(f\"å¹³å‡RÂ²ã‚¹ã‚³ã‚¢: {results['summary']['average_r2']:.3f}\")\n",
    "print(f\"ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆæ•°: {len(results['report_files'])}\")\n",
    "print(f\"ç”Ÿæˆå¯è¦–åŒ–æ•°: {len(results['visualization_files'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 çµæœã®è©³ç´°ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçµæœã®è©³ç´°è¡¨ç¤º\n",
    "if results['analysis_results']:\n",
    "    print(\"\\nğŸ¯ å•†å“åˆ¥åˆ†æçµæœ\")\n",
    "    for i, result in enumerate(results['analysis_results'][:3], 1):  # ä¸Šä½3å•†å“ã®ã¿è¡¨ç¤º\n",
    "        print(f\"\\n{i}. {result['product_name']}\")\n",
    "        print(f\"   å“è³ªãƒ¬ãƒ™ãƒ«: {result['quality_level']}\")\n",
    "        print(f\"   å®Ÿç”¨åŒ–æº–å‚™: {result['implementation_readiness']}\")\n",
    "        print(f\"   RÂ²ã‚¹ã‚³ã‚¢: {result['test_metrics']['r2_score']:.4f}\")\n",
    "        print(f\"   RMSE: {result['test_metrics']['rmse']:.2f}\")\n",
    "        \n",
    "        # éœ€è¦æ›²ç·šåˆ†æçµæœï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "        if 'demand_results' in result:\n",
    "            demand = result['demand_results']\n",
    "            current_price = demand.get('current_price', 0)\n",
    "            optimal_price = demand.get('optimal_price', 0)\n",
    "            if current_price > 0 and optimal_price > 0:\n",
    "                price_change = ((optimal_price - current_price) / current_price) * 100\n",
    "                print(f\"   ä¾¡æ ¼æœ€é©åŒ–: {current_price:.0f}å†† â†’ {optimal_price:.0f}å†† ({price_change:+.1f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸ åˆ†æå¯èƒ½ãªå•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½¿ç”¨\n",
    "\n",
    "### 3.1 ãƒ‡ãƒ¼ã‚¿å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½¿ç”¨ä¾‹\n",
    "config = Config(config_path)\n",
    "data_processor = DataProcessor(config)\n",
    "\n",
    "# ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "raw_data = data_processor.load_raw_data()\n",
    "print(f\"èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {len(raw_data)}è¡Œ x {len(raw_data.columns)}åˆ—\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º\n",
    "print(\"\\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:\")\n",
    "print(raw_data.info())\n",
    "\n",
    "# å•†å“åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°\n",
    "print(\"\\nğŸ·ï¸ å•†å“åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆä¸Šä½10å•†å“ï¼‰:\")\n",
    "product_counts = raw_data['å•†å“åç§°'].value_counts().head(10)\n",
    "print(product_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "print(\"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¸­...\")\n",
    "clean_data = data_processor.clean_data(raw_data)\n",
    "print(f\"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(clean_data)}è¡Œ\")\n",
    "\n",
    "# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®æ¯”è¼ƒ\n",
    "print(f\"å‰Šé™¤ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(raw_data) - len(clean_data)}\")\n",
    "print(f\"æ¬ æå€¤: {clean_data.isnull().sum().sum()}å€‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "feature_engineer = FeatureEngineer(config)\n",
    "\n",
    "print(\"âš™ï¸ ç‰¹å¾´é‡ç”Ÿæˆä¸­...\")\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡\n",
    "baseline_features = feature_engineer.create_baseline_features(clean_data)\n",
    "print(f\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡è¿½åŠ å¾Œ: {len(baseline_features.columns)}åˆ—\")\n",
    "\n",
    "# æ™‚é–“ç‰¹å¾´é‡\n",
    "time_features = feature_engineer.add_time_features(baseline_features)\n",
    "print(f\"æ™‚é–“ç‰¹å¾´é‡è¿½åŠ å¾Œ: {len(time_features.columns)}åˆ—\")\n",
    "\n",
    "# æ°—è±¡ç‰¹å¾´é‡ï¼ˆæ³¨æ„ï¼šAPIå‘¼ã³å‡ºã—ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "try:\n",
    "    final_features = feature_engineer.integrate_weather_features(time_features)\n",
    "    print(f\"æ°—è±¡ç‰¹å¾´é‡è¿½åŠ å¾Œ: {len(final_features.columns)}åˆ—\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ æ°—è±¡ç‰¹å¾´é‡è¿½åŠ ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    final_features = time_features\n",
    "\n",
    "# è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª\n",
    "new_columns = set(final_features.columns) - set(raw_data.columns)\n",
    "print(f\"\\nğŸ†• è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ ({len(new_columns)}å€‹):\")\n",
    "for col in sorted(new_columns):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å®šå•†å“ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¾‹\n",
    "model_builder = ModelBuilder(config)\n",
    "\n",
    "# åˆ†æå¯¾è±¡å•†å“ã‚’é¸æŠï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã®å¤šã„å•†å“ï¼‰\n",
    "product_counts = final_features['å•†å“åç§°'].value_counts()\n",
    "target_product = product_counts.index[0]  # æœ€ã‚‚å¤šã„å•†å“\n",
    "product_data = final_features[final_features['å•†å“åç§°'] == target_product].copy()\n",
    "\n",
    "print(f\"ğŸ¯ å¯¾è±¡å•†å“: {target_product}\")\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {len(product_data)}ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n",
    "\n",
    "if len(product_data) >= 10:  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯\n",
    "    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢\n",
    "    feature_columns = product_data.select_dtypes(include=['number']).columns.tolist()\n",
    "    target_column = 'æ•°é‡'\n",
    "    \n",
    "    if target_column in feature_columns:\n",
    "        feature_columns.remove(target_column)\n",
    "    \n",
    "    X = product_data[feature_columns]\n",
    "    y = product_data[target_column]\n",
    "    \n",
    "    print(f\"ç‰¹å¾´é‡æ•°: {len(feature_columns)}\")\n",
    "    print(f\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_column}\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
    "    print(\"\\nğŸ¤– ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...\")\n",
    "    model_results = model_builder.train_with_cv(X, y)\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    test_metrics = model_results['test_metrics']\n",
    "    print(f\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:\")\n",
    "    print(f\"RÂ²ã‚¹ã‚³ã‚¢: {test_metrics['r2_score']:.4f}\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    \n",
    "    # äº¤å·®æ¤œè¨¼çµæœ\n",
    "    cv_scores = model_results['cv_scores']\n",
    "    print(f\"\\nğŸ”„ äº¤å·®æ¤œè¨¼:\")\n",
    "    print(f\"å¹³å‡RÂ²: {cv_scores['mean_score']:.4f} Â± {cv_scores['std_score']:.4f}\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½5ä½ï¼‰\n",
    "    feature_importance = model_results['feature_importance']\n",
    "    print(f\"\\nğŸ† ç‰¹å¾´é‡é‡è¦åº¦ TOP5:\")\n",
    "    for i, (feature, importance) in enumerate(list(feature_importance.items())[:5], 1):\n",
    "        print(f\"{i}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹\n",
    "\n",
    "### 4.1 ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä¾‹\n",
    "custom_config = {\n",
    "    'model': {\n",
    "        'n_estimators': 50,  # æ±ºå®šæœ¨æ•°ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–\n",
    "        'max_depth': 5,      # æ·±åº¦ã‚’åˆ¶é™ã—ã¦éå­¦ç¿’é˜²æ­¢\n",
    "        'cv_folds': 3        # äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°ã‚’å‰Šæ¸›\n",
    "    },\n",
    "    'quality': {\n",
    "        'thresholds': {\n",
    "            'premium': 0.6,  # Premiumé–¾å€¤ã‚’ä¸‹ã’ã‚‹\n",
    "            'standard': 0.4,\n",
    "            'basic': 0.2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# è¨­å®šã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "import yaml\n",
    "custom_config_path = \"../config/custom_config.yaml\"\n",
    "with open(custom_config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(custom_config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(\"âš™ï¸ ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ\")\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "custom_pipeline = DemandForecastingPipeline(custom_config_path)\n",
    "custom_results = custom_pipeline.run_full_analysis(max_products=3)\n",
    "\n",
    "print(f\"ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œçµæœ: {custom_results['summary']['total_products_analyzed']}å•†å“åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ç‰¹å®šå•†å“ã®ã¿ã®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å®šå•†å“ã®ã¿åˆ†æ\n",
    "target_products = ['ã‚Šã‚“ã”', 'ã‚­ãƒ£ãƒ™ãƒ„']  # åˆ†æã—ãŸã„å•†å“ã‚’æŒ‡å®š\n",
    "\n",
    "print(f\"ğŸ¯ æŒ‡å®šå•†å“ã®ã¿åˆ†æ: {', '.join(target_products)}\")\n",
    "\n",
    "# æŒ‡å®šå•†å“ãŒå®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "available_products = final_features['å•†å“åç§°'].unique()\n",
    "valid_products = [p for p in target_products if p in available_products]\n",
    "\n",
    "if valid_products:\n",
    "    specific_results = pipeline.run_full_analysis(\n",
    "        target_products=valid_products,\n",
    "        max_products=len(valid_products)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… {len(specific_results['analysis_results'])}å•†å“ã®åˆ†æå®Œäº†\")\n",
    "    \n",
    "    for result in specific_results['analysis_results']:\n",
    "        print(f\"- {result['product_name']}: {result['quality_level']}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ æŒ‡å®šã•ã‚ŒãŸå•†å“ãŒãƒ‡ãƒ¼ã‚¿ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(f\"åˆ©ç”¨å¯èƒ½ãªå•†å“: {', '.join(available_products[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. çµæœã®åˆ†æã¨è§£é‡ˆ\n",
    "\n",
    "### 5.1 å“è³ªåˆ†å¸ƒã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçµæœã‹ã‚‰å“è³ªåˆ†å¸ƒã‚’æŠ½å‡º\n",
    "if results['analysis_results']:\n",
    "    quality_levels = [r['quality_level'] for r in results['analysis_results']]\n",
    "    r2_scores = [r['test_metrics']['r2_score'] for r in results['analysis_results']]\n",
    "    product_names = [r['product_name'] for r in results['analysis_results']]\n",
    "    \n",
    "    # å“è³ªåˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ\n",
    "    quality_counts = pd.Series(quality_levels).value_counts()\n",
    "    colors = {'Premium': '#27AE60', 'Standard': '#F39C12', 'Basic': '#E74C3C', 'Rejected': '#95A5A6'}\n",
    "    quality_colors = [colors.get(level, '#BDC3C7') for level in quality_counts.index]\n",
    "    \n",
    "    ax1.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%', \n",
    "            colors=quality_colors, startangle=90)\n",
    "    ax1.set_title('å“è³ªãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # RÂ²ã‚¹ã‚³ã‚¢åˆ†å¸ƒ\n",
    "    ax2.hist(r2_scores, bins=10, alpha=0.7, color='#FF6B35', edgecolor='black')\n",
    "    ax2.axvline(np.mean(r2_scores), color='red', linestyle='--', \n",
    "                label=f'å¹³å‡: {np.mean(r2_scores):.3f}')\n",
    "    ax2.set_xlabel('RÂ²ã‚¹ã‚³ã‚¢')\n",
    "    ax2.set_ylabel('å•†å“æ•°')\n",
    "    ax2.set_title('RÂ²ã‚¹ã‚³ã‚¢åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    print(f\"\\nğŸ“Š åˆ†æçµæœçµ±è¨ˆ:\")\n",
    "    print(f\"ç·å•†å“æ•°: {len(quality_levels)}\")\n",
    "    print(f\"å¹³å‡RÂ²ã‚¹ã‚³ã‚¢: {np.mean(r2_scores):.4f}\")\n",
    "    print(f\"RÂ²ã‚¹ã‚³ã‚¢æ¨™æº–åå·®: {np.std(r2_scores):.4f}\")\n",
    "    print(f\"æœ€é«˜RÂ²ã‚¹ã‚³ã‚¢: {max(r2_scores):.4f} ({product_names[r2_scores.index(max(r2_scores))]})\")\n",
    "    print(f\"æœ€ä½RÂ²ã‚¹ã‚³ã‚¢: {min(r2_scores):.4f} ({product_names[r2_scores.index(min(r2_scores))]})\")\n",
    "else:\n",
    "    print(\"ğŸ“Š åˆ†æçµæœãŒãªã„ãŸã‚å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "print(\"ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:\")\n",
    "\n",
    "print(\"\\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "for report_file in results.get('report_files', []):\n",
    "    file_path = Path(report_file)\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size\n",
    "        print(f\"  âœ… {file_path.name} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file_path.name} (ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n",
    "\n",
    "print(\"\\nğŸ¨ å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "for viz_file in results.get('visualization_files', []):\n",
    "    file_path = Path(viz_file)\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size\n",
    "        print(f\"  âœ… {file_path.name} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file_path.name} (ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n",
    "\n",
    "# Markdownãƒ¬ãƒãƒ¼ãƒˆã®å…ˆé ­éƒ¨åˆ†ã‚’è¡¨ç¤º\n",
    "md_files = [f for f in results.get('report_files', []) if f.endswith('.md')]\n",
    "if md_files:\n",
    "    print(f\"\\nğŸ“– Markdownãƒ¬ãƒãƒ¼ãƒˆæŠœç²‹ ({Path(md_files[0]).name}):\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        with open(md_files[0], 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            # æœ€åˆã®500æ–‡å­—ã®ã¿è¡¨ç¤º\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "### 6.1 ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­æ©Ÿèƒ½\n",
    "def system_diagnosis():\n",
    "    print(\"ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œä¸­...\\n\")\n",
    "    \n",
    "    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    data_path = \"../data/raw/ç´å“ç”¨_20240101_20241231_ç”Ÿé®®å…¨å“data.csv\"\n",
    "    if Path(data_path).exists():\n",
    "        print(\"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: å­˜åœ¨ã—ã¾ã™\")\n",
    "        file_size = Path(data_path).stat().st_size\n",
    "        print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes\")\n",
    "    else:\n",
    "        print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        print(f\"   æœŸå¾…ãƒ‘ã‚¹: {data_path}\")\n",
    "    \n",
    "    # 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    config_path = \"../config/config.yaml\"\n",
    "    if Path(config_path).exists():\n",
    "        print(\"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: å­˜åœ¨ã—ã¾ã™\")\n",
    "    else:\n",
    "        print(\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        example_path = \"../config/config.yaml.example\"\n",
    "        if Path(example_path).exists():\n",
    "            print(f\"   ğŸ’¡ ãƒ’ãƒ³ãƒˆ: {example_path} ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # 3. å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª\n",
    "    output_dirs = ['../reports', '../output/visualizations', '../logs']\n",
    "    for dir_path in output_dirs:\n",
    "        if Path(dir_path).exists():\n",
    "            print(f\"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {Path(dir_path).name}: å­˜åœ¨ã—ã¾ã™\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {Path(dir_path).name}: å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆè‡ªå‹•ä½œæˆã•ã‚Œã¾ã™ï¼‰\")\n",
    "    \n",
    "    # 4. Pythonç’°å¢ƒã®ç¢ºèª\n",
    "    print(f\"\\nğŸ Pythonç’°å¢ƒ:\")\n",
    "    print(f\"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version.split()[0]}\")\n",
    "    \n",
    "    # ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèª\n",
    "    required_packages = ['pandas', 'numpy', 'scikit-learn', 'matplotlib', 'seaborn']\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            exec(f\"import {package}\")\n",
    "            print(f\"   âœ… {package}: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿\")\n",
    "        except ImportError:\n",
    "            print(f\"   âŒ {package}: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦\")\n",
    "    \n",
    "    print(\"\\nğŸ” è¨ºæ–­å®Œäº†\")\n",
    "\n",
    "# è¨ºæ–­å®Ÿè¡Œ\n",
    "system_diagnosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ§ª ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ\\n\")\n",
    "\n",
    "# 1. å­˜åœ¨ã—ãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«\n",
    "try:\n",
    "    pipeline_error = DemandForecastingPipeline(\"nonexistent_config.yaml\")\n",
    "    print(\"âŒ å­˜åœ¨ã—ãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ… å­˜åœ¨ã—ãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: é©åˆ‡ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã•ã‚Œã¾ã—ãŸ\")\n",
    "    print(f\"   ã‚¨ãƒ©ãƒ¼: {type(e).__name__}\")\n",
    "\n",
    "# 2. ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã§ã®å‡¦ç†\n",
    "try:\n",
    "    # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®å‡¦ç†ãƒ†ã‚¹ãƒˆ\n",
    "    empty_df = pd.DataFrame()\n",
    "    processor = DataProcessor(Config())\n",
    "    result = processor.clean_data(empty_df)\n",
    "    print(f\"âœ… ç©ºãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†: æ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸï¼ˆçµæœ: {len(result)}è¡Œï¼‰\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ç©ºãƒ‡ãƒ¼ã‚¿å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}\")\n",
    "\n",
    "# 3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "print(f\"\\nğŸ’¾ ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_info.rss / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(\"\\nğŸ§ª ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ç”Ÿé®®é£Ÿå“éœ€è¦äºˆæ¸¬ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ğŸ¯ ä¸»è¦ãªå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ\n",
    "1. **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ** - `DemandForecastingPipeline`ã§ã®ä¸€æ‹¬åˆ†æ\n",
    "2. **å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ** - ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ç‰¹å¾´é‡ç”Ÿæˆã€ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã®å€‹åˆ¥ä½¿ç”¨\n",
    "3. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º** - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æŸ”è»Ÿãªèª¿æ•´\n",
    "4. **çµæœåˆ†æ** - å“è³ªè©•ä¾¡ã¨å¯è¦–åŒ–ã«ã‚ˆã‚‹çµæœã®è§£é‡ˆ\n",
    "5. **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°** - å•é¡Œè¨ºæ–­ã¨è§£æ±ºæ–¹æ³•\n",
    "\n",
    "### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "- å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã®æœ¬æ ¼é‹ç”¨\n",
    "- ã‚«ã‚¹ã‚¿ãƒ ç‰¹å¾´é‡ã®è¿½åŠ \n",
    "- ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- å®šæœŸå®Ÿè¡Œã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®è¨­å®š\n",
    "\n",
    "### ğŸ“ ã‚µãƒãƒ¼ãƒˆ\n",
    "æŠ€è¡“çš„ãªè³ªå•ã‚„ãƒˆãƒ©ãƒ–ãƒ«ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® Issues ã‹ã‚‰ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}